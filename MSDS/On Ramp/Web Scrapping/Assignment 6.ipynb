{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0f2bf6",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699a297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of roles\n",
      "Production Operator- Nokian Tyres\n",
      "Clinical Research Associate - Remote\n",
      "Laboratory Assistant\n",
      "Warehouse Worker\n",
      "Press Helper\n",
      "Food and Beverage Supervisor\n",
      "Guest Service Attendant\n",
      "Shipping & Receiving Professional\n",
      "Junior Camp Counselor - South City YMCA\n",
      "General Manager\n",
      "Dishwasher\n",
      "Assistant Superintendent\n",
      "General Laborer\n",
      "Core Technician\n",
      "CAD Drafter - Entry Level\n",
      "Multi-Store Supervisor - #655 - Evenings/Overnight\n",
      "Picker/Packer 1st Shift\n",
      "Microbiology Lab Assistant\n",
      "Greenskeeper\n",
      "Remote Data Entry Clerk No Experience\n",
      "Greenskeeper\n",
      "Patient Sitter\n",
      "Janitor/Day Porter/Cleaner\n",
      "Food Runner (FT/PT/Seasonal)\n",
      "Museum Technician.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "chromedriver_autoinstaller.install()  \n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3532460530')\n",
    "time.sleep(5)\n",
    "i=1\n",
    "roles=[]\n",
    "try:\n",
    "    while 1:\n",
    "        roles.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h3').text)\n",
    "        i=i+1\n",
    "except:\n",
    "    print(\"List of roles\")\n",
    "    for i in roles:\n",
    "        print(i)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47baa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Companies\n",
      "Alabama Department of Corrections\n",
      "Alabama Department of Corrections\n",
      "Northwell Health\n",
      "Tao Group Hospitality\n",
      "Cigna Healthcare\n",
      "Northwell Health\n",
      "Vivian Health\n",
      "Vivian Health\n",
      "Lifecare\n",
      "Cresa\n",
      "Vivian Health\n",
      "Cresa\n",
      "Cresa\n",
      "Sunstates Security\n",
      "ARO Liquidation Inc\n",
      "Cresa\n",
      "United States Postal Service\n",
      "University of Houston\n",
      "The Brothers that just do Gutters\n",
      "Kittitas Valley Healthcare\n",
      "Cresa\n",
      "WorkFello\n",
      "Northwell Health\n",
      "Northwell Health\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "chromedriver_autoinstaller.install()  \n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3532460530')\n",
    "time.sleep(5)\n",
    "i=1\n",
    "company=[]\n",
    "try:\n",
    "    while 1:\n",
    "        company.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h4').text)\n",
    "        i=i+1\n",
    "except:\n",
    "    print(\"List of Companies\")\n",
    "    for i in company:\n",
    "        print(i)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41819b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of locations\n",
      "United States\n",
      "Lubbock, TX\n",
      "Baton Rouge, LA\n",
      "Bozeman, MT\n",
      "Kent, WA\n",
      "Princeton, NJ\n",
      "Palm Desert, CA\n",
      "Little Rock, AR\n",
      "Estero, FL\n",
      "Washington, DC\n",
      "Rancho Santa Margarita, CA\n",
      "Shreve, OH\n",
      "Sonora, CA\n",
      "Bridgewater, NJ\n",
      "Jurupa Valley, CA\n",
      "Bend, OR\n",
      "New York, NY\n",
      "Dallas, TX\n",
      "Southampton, PA\n",
      "Las Vegas, NV\n",
      "Yonkers, NY\n",
      "Bradenton, FL\n",
      "Goodlettsville, TN\n",
      "Estero, FL\n",
      "Dover, DE\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "chromedriver_autoinstaller.install()  \n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3532460530')\n",
    "time.sleep(5)\n",
    "i=1\n",
    "location=[]\n",
    "try:\n",
    "    while 1:\n",
    "        location.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/div/span').text)\n",
    "        i=i+1\n",
    "except:\n",
    "    print(\"List of locations\")\n",
    "    for i in location:\n",
    "        print(i)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b570d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking of the company is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9c77f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of time\n",
      "2023-04-09\n",
      "2023-04-09\n",
      "2023-03-31\n",
      "2023-04-07\n",
      "2023-04-09\n",
      "2023-03-21\n",
      "2023-04-07\n",
      "2023-04-09\n",
      "2023-03-15\n",
      "2023-04-09\n",
      "2023-03-15\n",
      "2023-04-07\n",
      "2023-03-15\n",
      "2023-03-20\n",
      "2023-04-09\n",
      "2023-03-24\n",
      "2023-04-09\n",
      "2023-04-09\n",
      "2023-04-09\n",
      "2023-04-09\n",
      "2023-04-09\n",
      "2023-03-15\n",
      "2023-04-09\n",
      "2023-03-20\n",
      "2023-04-02\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "chromedriver_autoinstaller.install()  \n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3532460530')\n",
    "time.sleep(5)\n",
    "i=1\n",
    "time=[]\n",
    "try:\n",
    "    while 1:\n",
    "        time.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/div/time').get_attribute(\"datetime\"))\n",
    "        i=i+1\n",
    "except:\n",
    "    print(\"List of time\")\n",
    "    for i in time:\n",
    "        print(i)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb480da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "chromedriver_autoinstaller.install()  \n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3532460530')\n",
    "time.sleep(5)\n",
    "i=1\n",
    "time=[]\n",
    "location=[]\n",
    "company=[]\n",
    "roles=[]\n",
    "rows=[]\n",
    "try:\n",
    "    while 1:\n",
    "        location.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/div/span').text)\n",
    "        time.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/div/time').get_attribute(\"datetime\"))\n",
    "        company.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h4').text)\n",
    "        roles.append(driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h3').text)\n",
    "        i=i+1\n",
    "except:\n",
    "#     print(\"List of time\")\n",
    "    for i in range(len(time)):\n",
    "        rows.append([roles[i],company[i],location[i],\"\",time[i]])\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7968c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=[\"Title\",\"Company\",\"Location\",\"Rating\",\"Date Posted\"]\n",
    "import csv\n",
    "with open('job_1.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(fields)\n",
    "    write.writerows(rows)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0749c",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ef1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "class JobItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    company = scrapy.Field()\n",
    "    location = scrapy.Field()\n",
    "    \n",
    "\n",
    "class PythonDocumentationSpider(scrapy.Spider):\n",
    "    name = 'pydoc_bot'\n",
    "    allowed_domans=['linkedin.com']\n",
    "    start_urls = ['https://www.linkedin.com/jobs/search/?currentJobId=3532460530']\n",
    "    custom_settings = {\n",
    "                'FEEDS': {\n",
    "            'job_2.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        i=1\n",
    "\n",
    "        try:\n",
    "            while 1:\n",
    "                section= JobItem()\n",
    "                section['title'] = response.xpath('//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h3/text()').extract()[0].strip()\n",
    "                section['company'] = response.xpath('//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h4/a/text()').extract()[0].strip()\n",
    "                section['location']= response.xpath('//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/div/span/text()').extract()[0].strip()\n",
    "#                 print(section['company'])    \n",
    "                yield(section)\n",
    "                i=i+1\n",
    "        except:\n",
    "            i=i+1\n",
    "\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(PythonDocumentationSpider)\n",
    "    return d\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeaa443",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9cb5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class JobItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    company = scrapy.Field()\n",
    "    location = scrapy.Field()\n",
    "    \n",
    "\n",
    "class PythonDocumentationFollowingSpider(scrapy.Spider):\n",
    "    name = 'pydoc_bot'\n",
    "    allowed_domans=['linkedin.com']\n",
    "    start_urls = ['https://www.linkedin.com/jobs/search/?currentJobId=3532460530']\n",
    "    custom_settings = {\n",
    "                'FEEDS': {\n",
    "            'job_3.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    def parse(self, response):\n",
    "\n",
    "        follow_url = self.start_urls[0]           \n",
    "        yield scrapy.Request(follow_url, callback=self.parse_page_title)\n",
    "\n",
    "    def parse_page_title(self, response):\n",
    "        i=1\n",
    "\n",
    "        try:\n",
    "            while 1:\n",
    "                section= JobItem()\n",
    "                section['title'] = response.xpath('//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h3/text()').extract()[0].strip()\n",
    "                section['company'] = response.xpath('//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/h4/a/text()').extract()[0].strip()\n",
    "                section['location']= response.xpath('//*[@id=\"main-content\"]/section/ul/li['+str(i)+']/div/div[2]/div/span/text()').extract()[0].strip() \n",
    "                yield(section)\n",
    "                i=i+1\n",
    "        except:\n",
    "            i=i+1\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(PythonDocumentationFollowingSpider)\n",
    "    return d\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf90d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
